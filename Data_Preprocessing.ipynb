{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9060c3-0fcb-41f6-b3af-ac68f04916f7",
   "metadata": {},
   "source": [
    "# Data Processing Plan - Portfolio Project\n",
    "\n",
    "Student: Erdi Shpati  \n",
    "School: Holberton School  \n",
    "Date: November 15, 2025  \n",
    "Project: S&P 500 Preprocessing\n",
    "\n",
    "This notebook documents data sources, formats, features, exploration, hypotheses, missing data/outliers strategy, time-aware splits, bias mitigation, features for training, data types, transformations, and storage plans, aligned to the assignment requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba8bc93-59c2-40ca-8b83-d94354d30791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Admin/Downloads/archive/all_stocks_5yr.csv')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Local CSV path\n",
    "CSV_PATH = Path(r\"C:\\Users\\Admin\\Downloads\\archive\") / \"all_stocks_5yr.csv\"\n",
    "CSV_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a5f4b-d5d7-47b0-b3dc-98ea8fcf6d7a",
   "metadata": {},
   "source": [
    "## 1. Data Sources\n",
    "\n",
    "- Primary source: Local CSV located at C:\\Users\\Admin\\Downloads\\archive\\all_stocks_5yr.csv.  \n",
    "- Aggregation: Single source for baseline; may later augment with fundamentals or macro time series.  \n",
    "- Access method: pandas.read_csv using a Windows-safe path via pathlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb6bace-2a55-4c17-9429-8d94899001e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-02-08</th>\n",
       "      <td>15.07</td>\n",
       "      <td>15.12</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.75</td>\n",
       "      <td>8407500</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-11</th>\n",
       "      <td>14.89</td>\n",
       "      <td>15.01</td>\n",
       "      <td>14.26</td>\n",
       "      <td>14.46</td>\n",
       "      <td>8882000</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-12</th>\n",
       "      <td>14.45</td>\n",
       "      <td>14.51</td>\n",
       "      <td>14.10</td>\n",
       "      <td>14.27</td>\n",
       "      <td>8126000</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-13</th>\n",
       "      <td>14.30</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.25</td>\n",
       "      <td>14.66</td>\n",
       "      <td>10259500</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-14</th>\n",
       "      <td>14.94</td>\n",
       "      <td>14.96</td>\n",
       "      <td>13.16</td>\n",
       "      <td>13.99</td>\n",
       "      <td>31879900</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             open   high    low  close    volume Name\n",
       "date                                                 \n",
       "2013-02-08  15.07  15.12  14.63  14.75   8407500  AAL\n",
       "2013-02-11  14.89  15.01  14.26  14.46   8882000  AAL\n",
       "2013-02-12  14.45  14.51  14.10  14.27   8126000  AAL\n",
       "2013-02-13  14.30  14.94  14.25  14.66  10259500  AAL\n",
       "2013-02-14  14.94  14.96  13.16  13.99  31879900  AAL"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = Path(r\"C:\\Users\\Admin\\Downloads\\archive\") / \"all_stocks_5yr.csv\"\n",
    "\n",
    "# Read only first 1000 rows to keep memory tiny\n",
    "df = pd.read_csv(CSV_PATH, nrows=1000, parse_dates=[\"date\"])\n",
    "df = df.sort_values(\"date\").set_index(\"date\")\n",
    "print(df.shape); df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4c916-7aaf-4861-b632-d3112f11b7a7",
   "metadata": {},
   "source": [
    "## 2. Data Format\n",
    "\n",
    "- Current format: CSV with columns typically including date, open, high, low, close, volume, and a ticker identifier (Name).  \n",
    "- Target format: Pandas DataFrame indexed by datetime for time-series operations and Parquet for intermediate storage.  \n",
    "- Structure: Long format with multiple tickers stacked by date, enabling per-ticker rolling features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07cfdc05-2d1c-42e7-be64-8f3c51ff71dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['open', 'high', 'low', 'close', 'volume', 'Name']\n",
      "\n",
      "Null fraction (top 20):\n",
      "open      0.0\n",
      "high      0.0\n",
      "low       0.0\n",
      "close     0.0\n",
      "volume    0.0\n",
      "Name      0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nNull fraction (top 20):\")\n",
    "print(df.isna().mean().sort_values(ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aff3a1-4733-410f-890d-2b2085c4d4ea",
   "metadata": {},
   "source": [
    "## 3. Features and Exploration\n",
    "\n",
    "- Raw features: open, high, low, close, volume, Name, and date.  \n",
    "- EDA plan: descriptive statistics, missingness summary, distributions, correlations among numeric features, and per-ticker coverage over time.  \n",
    "- Derived features: 1-day/5-day returns, rolling means (volume MA5, close MA5/MA20), and simple momentum indicators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bbe70a5-1d1b-42d6-b55f-76ba7e3e7cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_model: (999, 13)\n"
     ]
    }
   ],
   "source": [
    "required = {\"open\",\"high\",\"low\",\"close\",\"volume\",\"Name\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "df[\"close_next\"] = df.groupby(\"Name\")[\"close\"].shift(-1)\n",
    "df[\"up_1d\"] = (df[\"close_next\"] > df[\"close\"]).astype(\"Int64\")\n",
    "df[\"ret_1d\"] = df.groupby(\"Name\")[\"close\"].pct_change()\n",
    "df[\"ret_5d\"] = df.groupby(\"Name\")[\"close\"].pct_change(5)\n",
    "df[\"vol_ma5\"] = df.groupby(\"Name\")[\"volume\"].transform(lambda s: s.rolling(5, min_periods=1).mean())\n",
    "df[\"close_ma5\"] = df.groupby(\"Name\")[\"close\"].transform(lambda s: s.rolling(5, min_periods=1).mean())\n",
    "df[\"close_ma20\"] = df.groupby(\"Name\")[\"close\"].transform(lambda s: s.rolling(20, min_periods=1).mean())\n",
    "\n",
    "df_model = df.dropna(subset=[\"close_next\"]).copy()\n",
    "print(\"df_model:\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294e438-5379-41e0-aece-c524bd45bf7e",
   "metadata": {},
   "source": [
    "## 4. Hypotheses and Testing\n",
    "\n",
    "- Hypothesis 1: Short-term moving averages (e.g., close MA5 vs. MA20) capture momentum informative for next-day direction.  \n",
    "  - Test: correlation/feature importance and evaluation by momentum regimes.  \n",
    "- Hypothesis 2: Recent returns and volume changes have predictive value for next-day up/down.  \n",
    "  - Test: classification metrics and calibration across feature quantiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf0e9503-57fd-44de-b7d8-47178a263312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has needed columns: True\n",
      "P(up_1d=1) by momentum regime (0=MA5<=MA20, 1=MA5>MA20):\n",
      "mom_pos\n",
      "0    0.548463\n",
      "1    0.522569\n",
      "Name: up_1d, dtype: Float64\n",
      "\n",
      "P(up_1d=1) by ret_1d quantile bin:\n",
      "ret_1d_bin\n",
      "(-0.0577, -0.0149]        0.465\n",
      "(-0.0149, -0.00312]    0.537688\n",
      "(-0.00312, 0.00698]        0.57\n",
      "(0.00698, 0.0188]      0.557789\n",
      "(0.0188, 0.0593]           0.54\n",
      "Name: up_1d, dtype: Float64\n",
      "\n",
      "Correlation with close_next (preview on first 20k rows):\n",
      "close_next    1.000000\n",
      "close         0.996818\n",
      "high          0.995688\n",
      "low           0.995546\n",
      "open          0.994181\n",
      "close_ma5     0.992252\n",
      "close_ma20    0.978314\n",
      "vol_ma5       0.224725\n",
      "volume        0.146502\n",
      "ret_5d        0.010956\n",
      "ret_1d        0.006913\n",
      "Name: close_next, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_17388\\1830105432.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  print(tmp.groupby(\"ret_1d_bin\")[\"up_1d\"].mean())\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sanity: ensure engineered features and targets exist\n",
    "needed_cols = {\"close_ma5\",\"close_ma20\",\"ret_1d\",\"ret_5d\",\"up_1d\",\"close_next\"}\n",
    "print(\"Has needed columns:\", needed_cols.issubset(set(df_model.columns)))\n",
    "\n",
    "# Hypothesis 1: Momentum via moving averages (close_ma5 vs close_ma20)\n",
    "# Create a simple momentum signal and check association with next-day direction (classification target up_1d)\n",
    "if {\"close_ma5\",\"close_ma20\",\"up_1d\"}.issubset(df_model.columns):\n",
    "    df_mom = df_model[[\"close_ma5\",\"close_ma20\",\"up_1d\"]].dropna().copy()\n",
    "    df_mom[\"mom_pos\"] = (df_mom[\"close_ma5\"] > df_mom[\"close_ma20\"]).astype(int)\n",
    "    # Group-wise accuracy proxy: fraction of positive next-day moves when momentum positive vs negative\n",
    "    grp = df_mom.groupby(\"mom_pos\")[\"up_1d\"].mean()\n",
    "    print(\"P(up_1d=1) by momentum regime (0=MA5<=MA20, 1=MA5>MA20):\")\n",
    "    print(grp)\n",
    "\n",
    "# Hypothesis 2: Recent returns and volume/price levels relate to next-day movement\n",
    "# Bin recent return ret_1d into quantiles and compare up_1d rates\n",
    "if {\"ret_1d\",\"up_1d\"}.issubset(df_model.columns):\n",
    "    tmp = df_model[[\"ret_1d\",\"up_1d\"]].dropna().copy()\n",
    "    # Use quantiles; clip extremes to reduce influence of outliers\n",
    "    tmp[\"ret_1d_clip\"] = tmp[\"ret_1d\"].clip(tmp[\"ret_1d\"].quantile(0.01), tmp[\"ret_1d\"].quantile(0.99))\n",
    "    tmp[\"ret_1d_bin\"] = pd.qcut(tmp[\"ret_1d_clip\"], q=5, duplicates=\"drop\")\n",
    "    print(\"\\nP(up_1d=1) by ret_1d quantile bin:\")\n",
    "    print(tmp.groupby(\"ret_1d_bin\")[\"up_1d\"].mean())\n",
    "\n",
    "# Optional: correlation preview for regression target (close_next) with engineered numeric features (sampled)\n",
    "numeric_checks = [\"open\",\"high\",\"low\",\"close\",\"volume\",\"ret_1d\",\"ret_5d\",\"vol_ma5\",\"close_ma5\",\"close_ma20\",\"close_next\"]\n",
    "numeric_checks = [c for c in numeric_checks if c in df_model.columns]\n",
    "if \"close_next\" in numeric_checks:\n",
    "    sample_n = min(len(df_model), 20000)\n",
    "    corr_preview = df_model[numeric_checks].iloc[:sample_n].corr()[\"close_next\"].sort_values(ascending=False)\n",
    "    print(\"\\nCorrelation with close_next (preview on first 20k rows):\")\n",
    "    print(corr_preview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0149f85-a669-4664-bfc5-64a6f838f786",
   "metadata": {},
   "source": [
    "## 5. Data Density, Missing Data, Outliers\n",
    "\n",
    "- Density: OHLCV is generally dense; verify and document any gaps.  \n",
    "- Missing handling: numeric → median imputation; categorical → most frequent in the pipeline for reproducibility.  \n",
    "- Outliers: detect with IQR/Z-score; choose whether to cap or keep due to market shock relevance, documenting the decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc281cbd-9c1d-4c5d-9e7c-564901d9963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top missing columns:\n",
      "ret_5d        0.005005\n",
      "ret_1d        0.001001\n",
      "low           0.000000\n",
      "high          0.000000\n",
      "open          0.000000\n",
      "volume        0.000000\n",
      "close         0.000000\n",
      "close_next    0.000000\n",
      "Name          0.000000\n",
      "up_1d         0.000000\n",
      "vol_ma5       0.000000\n",
      "close_ma5     0.000000\n",
      "close_ma20    0.000000\n",
      "dtype: float64\n",
      "IQR outlier rates (preview):\n",
      "volume        0.049049\n",
      "vol_ma5       0.042042\n",
      "ret_1d        0.028056\n",
      "ret_5d        0.025151\n",
      "high          0.000000\n",
      "open          0.000000\n",
      "low           0.000000\n",
      "close         0.000000\n",
      "close_ma5     0.000000\n",
      "close_ma20    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Missingness\n",
    "missing = df_model.isna().mean().sort_values(ascending=False)\n",
    "print(\"Top missing columns:\")\n",
    "print(missing.head(15))\n",
    "\n",
    "# Lightweight outlier rate (IQR) on compact numeric set\n",
    "core_num = [\n",
    "    \"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "    \"ret_1d\",\"ret_5d\",\"vol_ma5\",\"close_ma5\",\"close_ma20\"\n",
    "]\n",
    "core_num = [c for c in core_num if c in df_model.columns]\n",
    "\n",
    "def iqr_outlier_rate(s, factor=1.5):\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = q1 - factor*iqr, q3 + factor*iqr\n",
    "    return ((s < lo) | (s > hi)).mean()\n",
    "\n",
    "# Use train later for final rates; for now compute on full to preview\n",
    "preview = pd.Series({c: iqr_outlier_rate(df_model[c].dropna()) for c in core_num})\n",
    "print(\"IQR outlier rates (preview):\")\n",
    "print(preview.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5706458d-b525-43c6-8be5-eae1dbba401c",
   "metadata": {},
   "source": [
    "## 6. Data Splitting Strategy\n",
    "\n",
    "- Time-aware split by chronological order: 70% train, 15% validation, 15% test.  \n",
    "- Rationale: avoids look-ahead leakage and mirrors real predictive use.  \n",
    "- No shuffling; maintain the global timeline across all tickers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608345e9-5dc1-4735-a49f-bbaf019b4452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (700, 11) (150, 11) (149, 11)\n",
      "Preview cols: ['open', 'high', 'low', 'close', 'volume', 'ret_1d', 'ret_5d', 'vol_ma5', 'close_ma5', 'close_ma20', 'Name']\n"
     ]
    }
   ],
   "source": [
    "TARGET = \"up_1d\"  # or \"close_next\" for regression\n",
    "\n",
    "feature_cols = [\n",
    "    \"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "    \"ret_1d\",\"ret_5d\",\"vol_ma5\",\"close_ma5\",\"close_ma20\",\"Name\"\n",
    "]\n",
    "feature_cols = [c for c in feature_cols if c in df_model.columns]\n",
    "\n",
    "X = df_model[feature_cols].copy()\n",
    "y = df_model[TARGET].copy()\n",
    "\n",
    "# Time-aware split on tiny set\n",
    "all_idx = X.index.sort_values().unique()\n",
    "n = len(all_idx)\n",
    "cut1 = all_idx[int(n*0.70)] if n > 0 else None\n",
    "cut2 = all_idx[int(n*0.85)] if n > 0 else None\n",
    "\n",
    "train_idx = X.index[X.index <= cut1] if cut1 is not None else X.index[:0]\n",
    "val_idx   = X.index[(X.index > cut1) & (X.index <= cut2)] if cut2 is not None else X.index[:0]\n",
    "test_idx  = X.index[X.index > cut2] if cut2 is not None else X.index[:0]\n",
    "\n",
    "# If the sample is too small and any split is empty, fallback to simple head/tail split\n",
    "if len(train_idx) == 0 or len(val_idx) == 0:\n",
    "    split1 = int(len(X)*0.7)\n",
    "    split2 = int(len(X)*0.85)\n",
    "    train_idx = X.index[:split1]\n",
    "    val_idx   = X.index[split1:split2]\n",
    "    test_idx  = X.index[split2:]\n",
    "\n",
    "X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "X_val,   y_val   = X.loc[val_idx],   y.loc[val_idx]\n",
    "X_test,  y_test  = X.loc[test_idx],  y.loc[test_idx]\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Preview cols:\", X_train.columns[:20].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97459ebb-01f1-4efb-be6d-ec6966d9e176",
   "metadata": {},
   "source": [
    "## 7. Ensuring an Unbiased Dataset\n",
    "\n",
    "- Distribution alignment: ensure validation/test spans typical conditions; consider multiple validation windows if needed.  \n",
    "- Representation: verify a reasonable mix of tickers/sectors across splits; note any survivorship bias.  \n",
    "- Subgroup metrics: evaluate performance by ticker or sector and iterate on features to address gaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa4cbf6-13de-41a5-aba1-22ea2150158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tickers — Train/Val/Test: 1 1 1\n"
     ]
    }
   ],
   "source": [
    "if \"Name\" in X_train.columns:\n",
    "    print(\"Unique tickers — Train/Val/Test:\",\n",
    "          X_train[\"Name\"].nunique(),\n",
    "          X_val[\"Name\"].nunique(),\n",
    "          X_test[\"Name\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382a109-517b-40fd-bd77-fdb45c635e28",
   "metadata": {},
   "source": [
    "## 8. Features for Training\n",
    "\n",
    "- Initial features: OHLCV, returns (ret_1d, ret_5d), rolling means (vol_ma5, close_ma5, close_ma20), and Name as a categorical feature.  \n",
    "- Exclusions: any feature that leaks future information or redundant/unstable engineered fields.  \n",
    "- Future enhancements: volatility proxies, sector info, and macro covariates if justified.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced3533-adca-4ae5-94c5-1752c2716e4b",
   "metadata": {},
   "source": [
    "## 9. Data Types\n",
    "\n",
    "- Numerical: continuous OHLCV, returns, rolling stats.  \n",
    "- Categorical: Name (ticker), optionally sector.  \n",
    "- Datetime: used for chronological splitting and possible seasonal/cyclical features later.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6a025-fb80-4521-bd44-75ce7aef9526",
   "metadata": {},
   "source": [
    "## 10. Transformations and Pipeline\n",
    "\n",
    "- Numeric: median imputation and standardization.  \n",
    "- Categorical: most-frequent imputation and one-hot encoding for Name with handle_unknown=\"ignore\".  \n",
    "- Implemented with scikit-learn ColumnTransformer + Pipeline to ensure reproducibility and avoid leakage.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a927a7-8e24-4380-9907-118c8c58fde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: ['open', 'high', 'low', 'close', 'volume', 'ret_1d', 'ret_5d', 'vol_ma5', 'close_ma5', 'close_ma20']\n",
      "Categorical cols: ['Name']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_cols = [\n",
    "    \"open\",\"high\",\"low\",\"close\",\"volume\",\n",
    "    \"ret_1d\",\"ret_5d\",\"vol_ma5\",\"close_ma5\",\"close_ma20\"\n",
    "]\n",
    "num_cols = [c for c in num_cols if c in X_train.columns]\n",
    "cat_cols = [\"Name\"] if \"Name\" in X_train.columns else []\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                          (\"scaler\", StandardScaler())]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "print(\"Numeric cols:\", num_cols); print(\"Categorical cols:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058d909-4c3e-4dfb-bdd4-05a39576db44",
   "metadata": {},
   "source": [
    "## 11. Baseline Models and Metrics\n",
    "\n",
    "- Classification baseline: Logistic Regression with class_weight='balanced' (TARGET = up_1d).  \n",
    "- Regression baseline: Ridge Regression (TARGET = close_next).  \n",
    "- Evaluate on validation; keep test untouched for final estimate.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bddf4066-9fb7-48fb-995b-177f362034d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.67      0.57        70\n",
      "         1.0       0.58      0.40      0.47        80\n",
      "\n",
      "    accuracy                           0.53       150\n",
      "   macro avg       0.54      0.54      0.52       150\n",
      "weighted avg       0.54      0.53      0.52       150\n",
      "\n",
      "Validation ROC-AUC: 0.5183928571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([(\"prep\", preprocess),\n",
    "                (\"mdl\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))])\n",
    "clf.fit(X_train, y_train)\n",
    "val_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, val_pred))\n",
    "try:\n",
    "    val_proba = clf.predict_proba(X_val)[:, 1]\n",
    "    print(\"Validation ROC-AUC:\", roc_auc_score(y_val, val_proba))\n",
    "except Exception as e:\n",
    "    print(\"ROC-AUC not available:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214f44b-ac09-487a-9492-6d960eedafd0",
   "metadata": {},
   "source": [
    "## 12. Save Processed Data\n",
    "\n",
    "- Save a model-ready snapshot of the engineered dataset to Parquet for efficient storage and reloads.  \n",
    "- Store artifacts under data/processed with clear naming for reproducibility.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0234756-44db-456c-8b2a-fae2abb878fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Admin\\data\\processed\\sp500_all_stocks_5yr_model.parquet\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PROC_DIR = Path(\"data/processed\")\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_path = PROC_DIR / \"sp500_all_stocks_5yr_model.parquet\"\n",
    "df_model.to_parquet(out_path)\n",
    "print(\"Saved:\", out_path.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8dd22c-68d9-4ec4-9abd-0774a3021603",
   "metadata": {},
   "source": [
    "## Submission Summary\n",
    "\n",
    "- Loaded a local S&P 500 OHLCV dataset, parsed dates, and indexed chronologically to support time-series workflows.  \n",
    "- Engineered next-day targets (close_next for regression, up_1d for classification) and core numeric features (returns and rolling means) per ticker without leakage.  \n",
    "- Applied a chronological 70/15/15 train/validation/test split to avoid look-ahead bias and mirror real predictive use.  \n",
    "- Built a reproducible scikit-learn Pipeline that imputes/scales numeric features and one-hot encodes the ticker (Name) only within the Pipeline.  \n",
    "- Trained baseline models and evaluated on the validation set, with the test set reserved for final estimation; saved a model-ready Parquet snapshot for reproducibility.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
